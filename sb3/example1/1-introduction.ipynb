{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Terms\n",
    "\n",
    "#### Environment\n",
    "What are you trying to solve? : Fantasy Football Drafting\n",
    "\n",
    "#### Model: PPO\n",
    "\n",
    "#### Agent: \n",
    "The entity that interacts with the environment using an algorithm\n",
    "\n",
    "#### Observation (State): \n",
    "Important details of the environment that are fed to the model to make action predictions. (Vectors)\n",
    "\n",
    "#### Action: \n",
    "What the agent does in the environment. \"Pick QB Round 1\" Could be an example of an action\n",
    "\n",
    "#### Step: \n",
    "Progress in the environmeent. Can be thought of like FPS, where each \"frame\" is a step. \n",
    "In general a \"step\" in the environment will take an action for the agent to do, and return a new observation and reward for the step.\n",
    "\n",
    "#### Action Spaces, \n",
    "Discrete - Clear classifications (Go left or go right)\n",
    "Continuouse - Like regression (Go 0.02 right, or 0.5 or 0.34421; an infinite range; used in robotics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
    "# pip install stable-baselines3\n",
    "# pip install gymnasium\n",
    "# pip install shimmy < - no necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement learning is less about the algorithm. You can definately teak hyperparameters in your algorithm, but in general it is going to come down to your environment, your reward mechanism, and your observation\n",
    "# that you feed in to your algorithm. Of-course they take a long time to train. It will definately learn, changing the hyperparameter will not change too much, at least you can screw it up, but the defalt values is good enough.\n",
    "# You can eek out maybe an extra 5% - 10% performance possbily by tweaking hyperparameters but the actual gains are going to come much from algorithm + reward mechanisms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "DeprecatedEnv",
     "evalue": "Environment version v2 for `LunarLander` is deprecated. Please use `LunarLander-v3` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeprecatedEnv\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLunarLander-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 'human' for on-screen rendering\u001b[39;00m\n\u001b[1;32m      5\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Action: \u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gymnasium/envs/registration.py:687\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m     env_spec \u001b[38;5;241m=\u001b[39m \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gymnasium/envs/registration.py:531\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    525\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 531\u001b[0m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m     )\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gymnasium/envs/registration.py:431\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mVersionNotFound(message)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latest_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m version \u001b[38;5;241m<\u001b[39m latest_spec\u001b[38;5;241m.\u001b[39mversion:\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDeprecatedEnv(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment version v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_env_id(ns,\u001b[38;5;250m \u001b[39mname,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_spec\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m     )\n",
      "\u001b[0;31mDeprecatedEnv\u001b[0m: Environment version v2 for `LunarLander` is deprecated. Please use `LunarLander-v3` instead."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")  # 'human' for on-screen rendering\n",
    "\n",
    "env.reset()\n",
    "\n",
    "print(\"Sample Action: \", env.action_space.sample())\n",
    "print(\"Observation space shape: \", env.observation_space.shape)\n",
    "print(\"Sample Observation: \", env.observation_space.sample())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation is a flat vector; Action space value of some value that we can pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")  # 'human' for on-screen rendering\n",
    "env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    env.step(action)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.7209968941035525\n",
      "0.6309611213330868\n",
      "1.3893545117414317\n",
      "-3.0650638771274314\n",
      "0.782838231519321\n",
      "1.6946636320761013\n",
      "-4.443180653857257\n",
      "1.07915960659912\n",
      "1.9539777353993213\n",
      "1.3028523586336007\n",
      "-4.093596742394072\n",
      "0.4335571947000869\n",
      "0.08998179985292268\n",
      "-4.258824790837354\n",
      "0.8881893113424439\n",
      "0.8720805948562429\n",
      "0.8499337485937986\n",
      "-0.08327735965903457\n",
      "1.448532440636286\n",
      "-2.260400519646157\n",
      "-0.5529555116512472\n",
      "1.2237947627107826\n",
      "-1.6127487895232548\n",
      "-0.26065390396388355\n",
      "0.3878541326509435\n",
      "1.4623567299024114\n",
      "1.28783112204252\n",
      "-0.7375719180885187\n",
      "-0.9504422960529257\n",
      "-0.11995339961248419\n",
      "-0.22859769617255665\n",
      "-2.5395326474321562\n",
      "0.7424472395102566\n",
      "-1.2683813145451086\n",
      "-0.43731867684482495\n",
      "0.8856015751954953\n",
      "-1.5762734682805604\n",
      "-0.06513594453774657\n",
      "-1.7129236789162394\n",
      "0.09437963013809395\n",
      "-0.9241979842080639\n",
      "-1.155345332418807\n",
      "-2.156397575096963\n",
      "-2.629836624257069\n",
      "-0.04630437063245835\n",
      "-2.504228108844461\n",
      "-1.7034917449273508\n",
      "0.44513023694574483\n",
      "-2.353381323446274\n",
      "-0.29621210206283877\n",
      "-1.5767105071936942\n",
      "-0.647114130763299\n",
      "-1.800597253299826\n",
      "-0.9996897337398309\n",
      "0.2721582802132627\n",
      "0.15765181864445027\n",
      "-0.280958851519631\n",
      "-1.974839415427623\n",
      "-2.139156023809703\n",
      "-2.483682941700208\n",
      "-1.5270043895447145\n",
      "-0.2396635755203522\n",
      "-2.378005015293182\n",
      "-2.497092306060607\n",
      "-2.1192453459459157\n",
      "-0.5069461812836085\n",
      "-1.4293528936274242\n",
      "-2.0912278727152396\n",
      "-1.345856790703948\n",
      "-0.10997268818232328\n",
      "1.1277308290624546\n",
      "-1.2033859844868857\n",
      "0.02424097328423727\n",
      "0.12399849124656839\n",
      "0.090613883255686\n",
      "1.3282897818169317\n",
      "0.11940919913641437\n",
      "0.5307259825896267\n",
      "0.4524973720015748\n",
      "0.5005158584531284\n",
      "-0.20764677152425293\n",
      "1.1112712922963623\n",
      "0.6684644430124262\n",
      "0.9980018939175477\n",
      "-1.0251029589835514\n",
      "1.976119638636203\n",
      "2.138434794507174\n",
      "1.0689130029959426\n",
      "-0.5819060558419824\n",
      "3.7139069839785863\n",
      "0.9122502631159091\n",
      "0.1907195625352074\n",
      "0.20312354031301538\n",
      "1.2552121674903856\n",
      "-0.4440202592176081\n",
      "1.2617077919749147\n",
      "4.461657242390129\n",
      "1.3926544780273684\n",
      "1.437149449093938\n",
      "1.6216196186340721\n",
      "1.990801225048956\n",
      "1.9313110882472995\n",
      "-2.6462705063826193\n",
      "3.153173210313537\n",
      "1.0678448977290713\n",
      "-3.050865726010754\n",
      "-2.3279632762948084\n",
      "-3.0885120659837058\n",
      "-2.8988874520603063\n",
      "2.8350515457791916\n",
      "-2.658979440661709\n",
      "1.1253207526719506\n",
      "1.873956902806026\n",
      "2.501329190956011\n",
      "-2.4329825621384487\n",
      "0.3795551294864879\n",
      "-2.3706715500813957\n",
      "3.5892427927874566\n",
      "-2.8104875181161604\n",
      "3.266304126511568\n",
      "-2.5737136939066247\n",
      "-2.730931299609948\n",
      "-2.627960889403026\n",
      "-2.371247695352251\n",
      "-2.568697554450806\n",
      "-2.4867464917007838\n",
      "-2.6826427279515017\n",
      "0.5619456647901131\n",
      "-2.98152777791543\n",
      "2.654186805545902\n",
      "-3.298322570413886\n",
      "-3.2855582279114515\n",
      "-3.6443756061962334\n",
      "-0.3068253020918405\n",
      "-3.6287411140604036\n",
      "-3.6859168397233475\n",
      "-4.102764425710318\n",
      "-3.987954160787581\n",
      "-3.6672353768812216\n",
      "-3.95418446589224\n",
      "-4.060315272350294\n",
      "-4.711307786496461\n",
      "-0.7350260992316635\n",
      "-4.231613401076998\n",
      "-5.068306987904832\n",
      "-0.6393654403953974\n",
      "-4.970374886752836\n",
      "-4.487549911578042\n",
      "-2.0307891734340044\n",
      "-6.034598318940339\n",
      "-6.21605484356209\n",
      "56.5410647313291\n",
      "-16.015198398469437\n",
      "-22.459459318289362\n",
      "-32.594590895929855\n",
      "-21.50854511882078\n",
      "-100\n",
      "-2.316737145092047\n",
      "-1.4239981953642484\n",
      "0.4349810611189298\n",
      "-0.3942311162242891\n",
      "-2.42790343034295\n",
      "-0.4447625311886611\n",
      "-1.4039230988238955\n",
      "-1.3900144961053797\n",
      "-1.3742864259708654\n",
      "-2.459013837270389\n",
      "-0.09445067274599522\n",
      "-1.4704400507357889\n",
      "-2.200332380832718\n",
      "-2.76006167313855\n",
      "-2.711371158880523\n",
      "-1.940112455102593\n",
      "-1.9169926176321894\n",
      "-0.7816222886019932\n",
      "-2.4029705678988464\n",
      "-1.76025380177515\n",
      "-0.8280673751044947\n",
      "-0.6083668646817546\n",
      "-1.2922006400969224\n",
      "2.3670194910431066\n",
      "-2.5774642143893787\n",
      "-0.7247468825619297\n",
      "-1.3786240810612185\n",
      "-1.35316407418577\n",
      "-2.069373462973742\n",
      "-2.3731885028434747\n",
      "-0.7724563595586733\n",
      "-1.4115630289488763\n",
      "0.08882433722830002\n",
      "0.13134192242604287\n",
      "-0.6000217123393441\n",
      "-1.489837589305921\n",
      "-0.5932910127724813\n",
      "-0.5506632594409371\n",
      "-0.6029729127320025\n",
      "-1.138015732356223\n",
      "-2.2042971858946587\n",
      "-2.046255458181622\n",
      "-1.3936440781369583\n",
      "-0.12809829729211514\n",
      "-1.611676521304787\n",
      "-0.6938896456612429\n",
      "-1.563976768755765\n",
      "-2.766027019107382\n",
      "-2.9993532972549404\n",
      "-3.5201857997332966\n",
      "-2.8102185978225975\n",
      "-2.711658504486979\n",
      "-3.2743795081339613\n",
      "-3.5219873104792896\n",
      "-2.9594535824625994\n",
      "-4.957455247084272\n",
      "-2.9203690687472474\n",
      "-5.598437412662338\n",
      "118.18401840330331\n",
      "-2.034754018096123\n",
      "-17.20974194181545\n",
      "-28.828718655193768\n",
      "-18.358557439057364\n",
      "-18.385851837972154\n",
      "-18.992971376917154\n",
      "-18.66037850528724\n",
      "-18.833018963952583\n",
      "-19.01970701453814\n",
      "-24.38774670477817\n",
      "-100\n",
      "-1.318595263848947\n",
      "-2.671543711579761\n",
      "0.9488419905470391\n",
      "0.8292020660126422\n",
      "-0.012244343694760573\n",
      "-0.05598369943555781\n",
      "-1.0428578627675325\n",
      "0.999535885007732\n",
      "1.1341969448522502\n",
      "1.4315138571316492\n",
      "0.013513753342562052\n",
      "0.2384482041476872\n",
      "1.0440624660656976\n",
      "1.1182265573873462\n",
      "0.3445758192839321\n",
      "1.3331875191005633\n",
      "0.3833185860060837\n",
      "0.31274134448980817\n",
      "1.6592447444466825\n",
      "1.3275856386495672\n",
      "-0.43993861134467127\n",
      "0.23471018603797347\n",
      "-1.521116436770484\n",
      "-2.4567802155990237\n",
      "-0.48033881689920466\n",
      "-2.4626011318255494\n",
      "-2.1075485200223327\n",
      "2.7881869216352984\n",
      "-1.2051376927007595\n",
      "-0.575456281008029\n",
      "2.604854536357931\n",
      "-0.9872928398088436\n",
      "-1.69733517271942\n",
      "-1.3275501482709149\n",
      "-2.3156979630297188\n",
      "-2.1234664446966733\n",
      "-1.0816581421268257\n",
      "-2.262496639049941\n",
      "-1.2329507482349118\n",
      "-2.063356510264582\n",
      "-1.4669163021751785\n",
      "-1.4515398896541853\n",
      "-1.4334774865053816\n",
      "-1.4130487765768578\n",
      "-1.3904616917207022\n",
      "-1.6815442248997658\n",
      "4.498648651624632\n",
      "-1.1673754871358994\n",
      "4.545779571752331\n",
      "-1.1867162296460378\n",
      "-0.9462901942743815\n",
      "-1.578900569954867\n",
      "-1.3068774314239409\n",
      "-0.8530705104470826\n",
      "-1.0223839892654485\n",
      "-0.5272770953986867\n",
      "-0.7793341515671557\n",
      "-0.7474500921379672\n",
      "3.667530016296621\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Unpack the reset output (obs, info)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# Sample a random action\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Update to include 'truncated'\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gym/wrappers/env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gym/envs/box2d/lunar_lander.py:710\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    709\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    711\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "obs, info = env.reset()  # Unpack the reset output (obs, info)\n",
    "\n",
    "for _ in range(300):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()  # Sample a random action\n",
    "    obs, reward, done, truncated, info = env.step(action)  # Update to include 'truncated'\n",
    "    print(reward)\n",
    "    if done or truncated:  # Check for termination or truncation\n",
    "        obs, info = env.reset()  # Reset the environment\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 91       |\n",
      "|    ep_rew_mean        | -250     |\n",
      "| time/                 |          |\n",
      "|    fps                | 42       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.29    |\n",
      "|    explained_variance | -0.0151  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -20.3    |\n",
      "|    value_loss         | 375      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 107      |\n",
      "|    ep_rew_mean        | -334     |\n",
      "| time/                 |          |\n",
      "|    fps                | 44       |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | -0.00558 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -10.2    |\n",
      "|    value_loss         | 56.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 108      |\n",
      "|    ep_rew_mean        | -296     |\n",
      "| time/                 |          |\n",
      "|    fps                | 45       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.789   |\n",
      "|    explained_variance | -0.0477  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 3.29     |\n",
      "|    value_loss         | 33       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 115      |\n",
      "|    ep_rew_mean        | -258     |\n",
      "| time/                 |          |\n",
      "|    fps                | 45       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | -0.00261 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -8.83    |\n",
      "|    value_loss         | 39.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 121      |\n",
      "|    ep_rew_mean        | -268     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.808   |\n",
      "|    explained_variance | -0.0233  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.991    |\n",
      "|    value_loss         | 3.02     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 129      |\n",
      "|    ep_rew_mean        | -265     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 5.89e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -2.83    |\n",
      "|    value_loss         | 9.33     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 132      |\n",
      "|    ep_rew_mean        | -245     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.779   |\n",
      "|    explained_variance | -0.0158  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.234   |\n",
      "|    value_loss         | 3.26     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 135      |\n",
      "|    ep_rew_mean        | -239     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 86       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.0236   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -1.08    |\n",
      "|    value_loss         | 5.32     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 141      |\n",
      "|    ep_rew_mean        | -225     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 96       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.773   |\n",
      "|    explained_variance | 0.000249 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -4.9     |\n",
      "|    value_loss         | 27.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 142      |\n",
      "|    ep_rew_mean        | -219     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 107      |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.975   |\n",
      "|    explained_variance | 0.364    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 3.22     |\n",
      "|    value_loss         | 96.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 142      |\n",
      "|    ep_rew_mean        | -219     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 117      |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | -0.116   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00693 |\n",
      "|    value_loss         | 0.00347  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 169      |\n",
      "|    ep_rew_mean        | -204     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 128      |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.514   |\n",
      "|    explained_variance | 0.261    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -5.1     |\n",
      "|    value_loss         | 36.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 169      |\n",
      "|    ep_rew_mean        | -204     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | -0.0501  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.0865   |\n",
      "|    value_loss         | 0.0224   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 192      |\n",
      "|    ep_rew_mean        | -192     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 149      |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.708   |\n",
      "|    explained_variance | -0.0616  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -3.53    |\n",
      "|    value_loss         | 68       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 192      |\n",
      "|    ep_rew_mean        | -185     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 159      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | -0.0475  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -6.62    |\n",
      "|    value_loss         | 1.33e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 194      |\n",
      "|    ep_rew_mean        | -181     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 170      |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.634   |\n",
      "|    explained_variance | -0.265   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 5.37     |\n",
      "|    value_loss         | 63.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 195      |\n",
      "|    ep_rew_mean        | -177     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 180      |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.78    |\n",
      "|    explained_variance | 0.0173   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -9.42    |\n",
      "|    value_loss         | 256      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 195      |\n",
      "|    ep_rew_mean        | -178     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 191      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.637   |\n",
      "|    explained_variance | 0.0383   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -1.23    |\n",
      "|    value_loss         | 41.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 193      |\n",
      "|    ep_rew_mean        | -171     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 202      |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.317   |\n",
      "|    explained_variance | -0.00621 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -5.55    |\n",
      "|    value_loss         | 19.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 190      |\n",
      "|    ep_rew_mean        | -168     |\n",
      "| time/                 |          |\n",
      "|    fps                | 46       |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 212      |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.529   |\n",
      "|    explained_variance | -0.0368  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.155   |\n",
      "|    value_loss         | 0.0646   |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "obs, info = env.reset()  # Unpack the reset output (obs, info)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        env.render()\n",
    "        action = env.action_space.sample()  # Sample a random action\n",
    "        obs, reward, done, truncated, info = env.step(action)  # Update to include 'truncated'\n",
    "        # print(reward)\n",
    "        if done or truncated:  # Check for termination or truncation\n",
    "            obs, info = env.reset()  # Reset the environment\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_mean - how long it survives\n",
    "# rew_mean - reward mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.8     |\n",
      "|    ep_rew_mean     | -168     |\n",
      "| time/              |          |\n",
      "|    fps             | 44       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 96         |\n",
      "|    ep_rew_mean          | -165       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 45         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00911714 |\n",
      "|    clip_fraction        | 0.0413     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.00443    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 453        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00971   |\n",
      "|    value_loss           | 1.32e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 97.5        |\n",
      "|    ep_rew_mean          | -175        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008316039 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.0107     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 432         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    value_loss           | 1.03e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99.8        |\n",
      "|    ep_rew_mean          | -165        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006967605 |\n",
      "|    clip_fraction        | 0.0495      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.00525    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 341         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    value_loss           | 1.12e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 101         |\n",
      "|    ep_rew_mean          | -162        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 222         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013579818 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.00105    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 206         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 654         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "obs, info = env.reset()  # Unpack the reset output (obs, info)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        env.render()\n",
    "        action = env.action_space.sample()  # Sample a random action\n",
    "        obs, reward, done, truncated, info = env.step(action)  # Update to include 'truncated'\n",
    "        # print(reward)\n",
    "        if done or truncated:  # Check for termination or truncation\n",
    "            obs, info = env.reset()  # Reset the environment\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.5     |\n",
      "|    ep_rew_mean     | -220     |\n",
      "| time/              |          |\n",
      "|    fps             | 46       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Unpack the reset output (obs, info)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gymnasium/envs/box2d/lunar_lander.py:665\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    662\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 665\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/draftai/lib/python3.9/site-packages/gymnasium/envs/box2d/lunar_lander.py:779\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 779\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    782\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    783\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "obs, info = env.reset()  # Unpack the reset output (obs, info)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        env.render()\n",
    "        action = env.action_space.sample()  # Sample a random action\n",
    "        obs, reward, done, truncated, info = env.step(action)  # Update to include 'truncated'\n",
    "        # print(reward)\n",
    "        if done or truncated:  # Check for termination or truncation\n",
    "            obs, info = env.reset()  # Reset the environment\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "draftai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
